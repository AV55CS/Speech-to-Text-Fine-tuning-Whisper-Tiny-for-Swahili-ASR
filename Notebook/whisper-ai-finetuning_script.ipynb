{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":10844502,"datasetId":6734942,"databundleVersionId":11207232},{"sourceType":"kernelVersion","sourceId":223439286},{"sourceType":"kernelVersion","sourceId":224420289},{"sourceType":"kernelVersion","sourceId":224595748}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch evaluate tensorboard","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\n# Define the specific paths for your Kaggle environment\nCSV_PATH = '/kaggle/input/swahili-csv/processed_swahili_audio.csv'\nAUDIO_DIR = '/kaggle/input/preprocessor/'\nOUTPUT_DIR = '/kaggle/working/analysis'\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Load the dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv(CSV_PATH)\nprint(f\"Dataset loaded with {len(df)} samples\")\n\n# Display basic info\nprint(\"\\nDataset Info:\")\nprint(df.info())\n\n# Display sample rows\nprint(\"\\nSample data:\")\nprint(df.head())\n\n# Basic statistics\nprint(\"\\nStatistical Summary:\")\nprint(df.describe())\n\n# Check for missing values\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum())\n\n# Visualize transcript length distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df['Transcript_Length'], kde=True, bins=30)\nplt.title('Distribution of Transcript Lengths')\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency')\nplt.axvline(df['Transcript_Length'].mean(), color='r', linestyle='--', label=f'Mean: {df[\"Transcript_Length\"].mean():.2f}')\nplt.axvline(df['Transcript_Length'].median(), color='g', linestyle='--', label=f'Median: {df[\"Transcript_Length\"].median():.2f}')\nplt.legend()\nplt.savefig(f'{OUTPUT_DIR}/transcript_length_dist.png')\n\n# Visualize token length distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df['Token_Length'], kde=True, bins=30)\nplt.title('Distribution of Token Lengths')\nplt.xlabel('Number of Tokens')\nplt.ylabel('Frequency')\nplt.axvline(df['Token_Length'].mean(), color='r', linestyle='--', label=f'Mean: {df[\"Token_Length\"].mean():.2f}')\nplt.axvline(df['Token_Length'].median(), color='g', linestyle='--', label=f'Median: {df[\"Token_Length\"].median():.2f}')\nplt.legend()\nplt.savefig(f'{OUTPUT_DIR}/token_length_dist.png')\n\n# Create transcript length categories\nbins = [0, 10, 20, 30, 40, 50, 100]\nlabels = ['1-10', '11-20', '21-30', '31-40', '41-50', '51+']\ndf['Length_Category'] = pd.cut(df['Transcript_Length'], bins=bins, labels=labels)\n\n# Plot category distribution\nplt.figure(figsize=(10, 6))\ncategory_counts = df['Length_Category'].value_counts().sort_index()\nsns.barplot(x=category_counts.index, y=category_counts.values)\nplt.title('Distribution of Transcript Length Categories')\nplt.xlabel('Word Count Range')\nplt.ylabel('Number of Samples')\nfor i, count in enumerate(category_counts):\n    plt.text(i, count + 5, f'{count} ({count/len(df)*100:.1f}%)', ha='center')\nplt.savefig(f'{OUTPUT_DIR}/length_categories.png')\n\n# Print implications for fine-tuning\nprint(\"\\nImplications for Fine-tuning:\")\nprint(f\"Total samples: {len(df)}\")\nprint(f\"Training set size (80%): {int(len(df)*0.8)} samples\")\nprint(f\"Validation set size (20%): {int(len(df)*0.2)} samples\")\nprint(f\"Average transcript length: {df['Transcript_Length'].mean():.2f} words\")\nprint(f\"Maximum transcript length: {df['Transcript_Length'].max()} words\")\nprint(f\"Average token length: {df['Token_Length'].mean():.2f} tokens\")\nprint(f\"Maximum token length: {df['Token_Length'].max()} tokens\")\n\n# Check a few audio files to confirm they exist\nprint(\"\\nChecking audio file paths...\")\naudio_files = df['File'].values[:5]  # Check first 5 files\nfor file in audio_files:\n    full_path = os.path.join(AUDIO_DIR, os.path.basename(file))\n    exists = os.path.exists(full_path)\n    print(f\"File: {os.path.basename(file)}, Exists: {exists}\")\n\n# Save analysis summary\nsummary = {\n    \"total_samples\": len(df),\n    \"training_samples\": int(len(df)*0.8),\n    \"validation_samples\": int(len(df)*0.2),\n    \"avg_transcript_length\": df['Transcript_Length'].mean(),\n    \"max_transcript_length\": df['Transcript_Length'].max(),\n    \"avg_token_length\": df['Token_Length'].mean(),\n    \"max_token_length\": df['Token_Length'].max(),\n    \"transcript_distribution\": df['Length_Category'].value_counts(normalize=True).sort_index().to_dict()\n}\n\n# Save to file\nimport json\nwith open(f'{OUTPUT_DIR}/analysis_summary.json', 'w') as f:\n    json.dump(summary, f, indent=4)\n\nprint(f\"\\nAnalysis complete. Results saved to {OUTPUT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetuning Procedure and Training","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport evaluate\n\n# Install jiwer first\n!pip install jiwer\n\n# Define paths\nCSV_PATH = '/kaggle/input/swahili-csv/processed_swahili_audio.csv'\nAUDIO_BASE_DIR = '/kaggle/input/preprocessor/'\nOUTPUT_DIR = '/kaggle/working/whisper-tiny-swahili-new'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Custom dataset with robust audio processing\nclass SwahiliWhisperDataset(Dataset):\n    def __init__(self, df, processor, split=\"train\"):\n        self.df = df\n        self.processor = processor\n        self.split = split\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        try:\n            # Get the file path and transcript\n            file_path = self.df.iloc[idx][\"File\"]\n            transcript = self.df.iloc[idx][\"Original_Transcript\"]\n            \n            # Construct the correct audio path\n            audio_path = os.path.join(AUDIO_BASE_DIR, file_path)\n            \n            # Load audio with librosa directly\n            waveform, sample_rate = librosa.load(audio_path, sr=16000)\n            \n            # Process inputs\n            inputs = self.processor(\n                waveform, \n                sampling_rate=sample_rate, \n                return_tensors=\"pt\"\n            )\n            \n            # Get input features - no need to handle attention_mask for Whisper\n            input_features = inputs.input_features.squeeze()\n            \n            # Process labels\n            labels = self.processor.tokenizer(transcript).input_ids\n            \n            return {\n                \"input_features\": input_features,\n                \"labels\": labels\n            }\n        except Exception as e:\n            import traceback\n            error_msg = f\"Error processing {idx} - {type(e).__name__}: {str(e)}\"\n            print(error_msg)\n            traceback.print_exc()  # Print the full stack trace\n            \n            # Return a placeholder\n            return {\n                \"input_features\": torch.zeros((80, 3000), dtype=torch.float32),\n                \"labels\": torch.tensor([0])\n            }\n\n# Improved data collator\ndef data_collator(features):\n    # Filter out error samples\n    valid_features = []\n    for feature in features:\n        if isinstance(feature[\"input_features\"], torch.Tensor) and feature[\"input_features\"].ndim == 2:\n            valid_features.append(feature)\n    \n    if len(valid_features) == 0:\n        return {\n            \"input_features\": torch.zeros((1, 80, 3000), dtype=torch.float32),\n            \"labels\": torch.tensor([[0]])\n        }\n    \n    # Stack input features\n    try:\n        input_features = torch.stack([feature[\"input_features\"] for feature in valid_features])\n    except RuntimeError as e:\n        print(f\"Error stacking features: {e}\")\n        shapes = [feature[\"input_features\"].shape for feature in valid_features]\n        print(f\"Feature shapes: {shapes}\")\n        # Return a dummy batch\n        return {\n            \"input_features\": torch.zeros((len(valid_features), 80, 3000), dtype=torch.float32),\n            \"labels\": torch.tensor([[0]] * len(valid_features))\n        }\n    \n    # Process labels\n    try:\n        labels = processor.tokenizer.pad(\n            [{\"input_ids\": feature[\"labels\"]} for feature in valid_features],\n            return_tensors=\"pt\",\n        )[\"input_ids\"]\n    except Exception as e:\n        print(f\"Error padding labels: {e}\")\n        # Create dummy labels\n        labels = torch.ones((len(valid_features), 1), dtype=torch.long)\n    \n    return {\n        \"input_features\": input_features,\n        \"labels\": labels,\n    }\n\n# Compute metrics function\ndef compute_metrics(pred):\n    pred_ids = pred.predictions\n    label_ids = pred.label_ids\n    \n    # Replace -100 with pad token id\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    \n    # Decode predictions and references\n    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n    \n    # Compute WER\n    wer_metric = evaluate.load(\"wer\")\n    wer_score = wer_metric.compute(predictions=pred_str, references=label_str)\n    \n    return {\"wer\": wer_score}\n\n# Main training function\ndef train_whisper_tiny():\n    # Load dataset\n    print(\"Loading dataset...\")\n    df = pd.read_csv(CSV_PATH)\n    print(f\"Dataset loaded with {len(df)} samples\")\n    \n    # Create train/eval split\n    train_df = df.sample(frac=0.8, random_state=42)\n    eval_df = df.drop(train_df.index)\n    print(f\"Training samples: {len(train_df)}, Evaluation samples: {len(eval_df)}\")\n    \n    # Load model and processor\n    print(\"Loading Whisper Tiny model...\")\n    global processor\n    model_name = \"openai/whisper-tiny\"\n    processor = WhisperProcessor.from_pretrained(model_name)\n    model = WhisperForConditionalGeneration.from_pretrained(model_name)\n    \n    # Set forced decoder ids for Swahili\n    model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n        language=\"swahili\", task=\"transcribe\"\n    )\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset = SwahiliWhisperDataset(train_df, processor, split=\"train\")\n    eval_dataset = SwahiliWhisperDataset(eval_df, processor, split=\"eval\")\n    \n    # Configure training arguments\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=OUTPUT_DIR,\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=4,\n        learning_rate=5e-5,\n        num_train_epochs=5,\n        warmup_ratio=0.1,\n        gradient_checkpointing=True,\n        fp16=True,\n        eval_strategy=\"steps\",\n        per_device_eval_batch_size=8,\n        predict_with_generate=True,\n        generation_max_length=225,\n        save_steps=500,\n        eval_steps=500,\n        logging_steps=50,\n        report_to=[\"tensorboard\"],\n        load_best_model_at_end=True,\n        metric_for_best_model=\"wer\",\n        greater_is_better=False,\n        push_to_hub=False,\n    )\n    \n    # Create trainer with processing_class instead of tokenizer\n    trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n        tokenizer=processor.tokenizer,  # Will show deprecation warning but works\n    )\n    \n    # Train the model\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    \n    # Save the model\n    model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n    processor.save_pretrained(os.path.join(OUTPUT_DIR, \"final_processor\"))\n    print(f\"Model saved to {OUTPUT_DIR}\")\n    \n    return model, processor\n\n# Run the training function\nif __name__ == \"__main__\":\n    train_whisper_tiny()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evalautaion script","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport pandas as pd\nimport librosa\nimport evaluate\nfrom tqdm.auto import tqdm\n\n# Paths\nMODEL_DIR = '/kaggle/working/whisper-tiny-swahili-new/final_model'\nPROCESSOR_DIR = '/kaggle/working/whisper-tiny-swahili-new/final_processor'\nCSV_PATH = '/kaggle/input/swahili-csv/processed_swahili_audio.csv'\nAUDIO_BASE_DIR = '/kaggle/input/preprocessor/'\n\n# Load model and processor\nprint(\"Loading final model and processor...\")\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_DIR)\nprocessor = WhisperProcessor.from_pretrained(PROCESSOR_DIR)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nprint(f\"Model loaded on {device}\")\n\n# Load WER metric\nwer_metric = evaluate.load(\"wer\")\n\n# Create a test set (use a different subset than training)\nprint(\"Loading dataset...\")\ndf = pd.read_csv(CSV_PATH)\n# Create a test set (20% of data, different random seed than training)\ntest_df = df.sample(frac=0.2, random_state=24)  # Different seed from 42\nprint(f\"Test samples: {len(test_df)}\")\n\n# Run evaluation on a sample of test data\nprint(\"Starting evaluation...\")\nsample_size = min(100, len(test_df))  # Evaluate on up to 100 samples\ntest_sample = test_df.head(sample_size)\npredictions = []\nreferences = []\nerrors = []\n\nfor idx, row in tqdm(test_sample.iterrows(), total=len(test_sample)):\n    try:\n        # Get file path and transcript\n        file_path = row[\"File\"]\n        transcript = row[\"Original_Transcript\"]\n        audio_path = os.path.join(AUDIO_BASE_DIR, file_path)\n        \n        # Load and process audio\n        waveform, sample_rate = librosa.load(audio_path, sr=16000)\n        \n        # Get model prediction\n        input_features = processor(\n            waveform, \n            sampling_rate=sample_rate, \n            return_tensors=\"pt\"\n        ).input_features.to(device)\n        \n        # Generate transcription\n        forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"swahili\", task=\"transcribe\")\n        with torch.no_grad():\n            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n        \n        # Decode prediction\n        prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n        \n        predictions.append(prediction)\n        references.append(transcript)\n    except Exception as e:\n        errors.append((idx, str(e)))\n\n# Calculate WER\nif predictions:\n    wer = wer_metric.compute(predictions=predictions, references=references)\n    print(f\"Word Error Rate (WER): {wer:.4f}\")\nelse:\n    print(\"No successful predictions to evaluate\")\n\n# Save results to file\nwith open('/kaggle/working/final_evaluation_results.txt', 'w') as f:\n    f.write(f\"Final Model Evaluation Results\\n\")\n    f.write(f\"---------------------------\\n\")\n    f.write(f\"Test samples evaluated: {len(predictions)}\\n\")\n    \n    # Fixed the f-string error\n    if 'wer' in locals():\n        f.write(f\"Word Error Rate (WER): {wer:.4f}\\n\\n\")\n    else:\n        f.write(\"Word Error Rate (WER): N/A\\n\\n\")\n    \n    f.write(f\"Sample predictions (first 10):\\n\")\n    for i in range(min(10, len(predictions))):\n        f.write(f\"\\nReference: {references[i]}\\n\")\n        f.write(f\"Prediction: {predictions[i]}\\n\")\n        \n    if errors:\n        f.write(f\"\\nErrors during evaluation: {len(errors)}\\n\")\n        for idx, error in errors[:10]:  # Show first 10 errors\n            f.write(f\"Sample {idx}: {error}\\n\")\n\nprint(f\"Evaluation results saved to /kaggle/working/final_evaluation_results.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport librosa\n\n# Path to your saved partial model\nMODEL_DIR = '/kaggle/working/whisper-tiny-swahili/final_model'\nPROCESSOR_DIR = '/kaggle/working/whisper-tiny-swahili/final_processor'\n\n# Load the model and processor\nprint(\"Loading model and processor...\")\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_DIR)\nprocessor = WhisperProcessor.from_pretrained(PROCESSOR_DIR)\n\n# Set device (GPU if available)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = model.to(device)\nprint(f\"Model loaded on {device}\")\n\n# Simple function to test on audio files\ndef test_on_file(audio_path, expected_transcript=None):\n    print(f\"\\nTesting on: {audio_path}\")\n    \n    # Load audio\n    waveform, sample_rate = librosa.load(audio_path, sr=16000)\n    print(f\"Audio loaded, length: {len(waveform)/16000:.2f} seconds\")\n    \n    # Process features\n    input_features = processor(\n        waveform, \n        sampling_rate=sample_rate, \n        return_tensors=\"pt\"\n    ).input_features.to(device)\n    \n    # Generate transcription\n    forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"swahili\", task=\"transcribe\")\n    predicted_ids = model.generate(\n        input_features, \n        forced_decoder_ids=forced_decoder_ids,\n        max_length=225\n    )\n    \n    # Decode\n    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n    \n    print(f\"Model transcription: {transcription}\")\n    \n    # Compare with expected transcript if provided\n    if expected_transcript:\n        print(f\"Expected transcript: {expected_transcript}\")\n        # Simple word match count for a rough accuracy measure\n        pred_words = set(transcription.lower().split())\n        expected_words = set(expected_transcript.lower().split())\n        overlap = pred_words.intersection(expected_words)\n        accuracy = len(overlap) / len(expected_words) if expected_words else 0\n        print(f\"Word overlap accuracy: {accuracy:.2%}\")\n    \n    return transcription\n\n# Test on a few sample files from your dataset\n# These are files we've confirmed exist\ntest_files = [\n    \"/kaggle/input/preprocessor/converted_wav_files/sample_261.wav\",\n    \"/kaggle/input/preprocessor/converted_wav_files/sample_262.wav\",\n    \"/kaggle/input/preprocessor/converted_wav_files/sample_263.wav\"\n]\n\n# Get the corresponding transcripts from your CSV file\nimport pandas as pd\ndf = pd.read_csv('/kaggle/input/swahili-csv/processed_swahili_audio.csv')\n\n# Create a dictionary to look up transcripts by filename\ntranscript_dict = {}\nfor _, row in df.iterrows():\n    filename = os.path.basename(row['File'])\n    transcript_dict[filename] = row['Original_Transcript']\n\nprint(\"Starting model evaluation on sample files...\")\nresults = []\n\nfor file in test_files:\n    filename = os.path.basename(file)\n    expected_transcript = transcript_dict.get(filename, None)\n    result = test_on_file(file, expected_transcript)\n    results.append((filename, result, expected_transcript))\n    print(\"-\" * 50)\n\n# Print summary of results\nprint(\"\\nSummary of test results:\")\nfor filename, prediction, expected in results:\n    print(f\"File: {filename}\")\n    print(f\"Prediction: {prediction}\")\n    print(f\"Expected: {expected}\")\n    print(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optional Evalaution (CER)","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\nimport pandas as pd\nimport librosa\nimport numpy as np\nimport evaluate\nfrom tqdm.auto import tqdm\n\n# Paths\nMODEL_DIR = '/kaggle/input/swahili-data-analysis/whisper-tiny-swahili/final_model'\nPROCESSOR_DIR = '/kaggle/input/swahili-data-analysis/whisper-tiny-swahili/final_processor/'\nCSV_PATH = '/kaggle/input/swahili-csv/processed_swahili_audio.csv'\nAUDIO_BASE_DIR = '/kaggle/input/preprocessor/'\n\n\n# Load model and processor\nprint(\"Loading model and processor...\")\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_DIR)\nprocessor = WhisperProcessor.from_pretrained(PROCESSOR_DIR)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\nprint(f\"Model loaded on {device}\")\n\n# Load WER metric\nwer_metric = evaluate.load(\"wer\")\n\n# Load dataset\nprint(\"Loading dataset...\")\ndf = pd.read_csv(CSV_PATH)\n# Use the same validation split as during training\neval_df = df.drop(df.sample(frac=0.8, random_state=42).index)\nprint(f\"Validation samples: {len(eval_df)}\")\n\n# Run evaluation\nprint(\"Starting evaluation...\")\npredictions = []\nreferences = []\nerrors = []\n\nfor idx, row in tqdm(eval_df.iterrows(), total=len(eval_df)):\n    try:\n        # Get file path and transcript\n        file_path = row[\"File\"]\n        transcript = row[\"Original_Transcript\"]\n        audio_path = os.path.join(AUDIO_BASE_DIR, file_path)\n        \n        # Load and process audio\n        waveform, sample_rate = librosa.load(audio_path, sr=16000)\n        \n        # Get model prediction\n        input_features = processor(\n            waveform, \n            sampling_rate=sample_rate, \n            return_tensors=\"pt\"\n        ).input_features.to(device)\n        \n        # Generate transcription\n        forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"swahili\", task=\"transcribe\")\n        with torch.no_grad():\n            predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n        \n        # Decode prediction\n        prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n        \n        predictions.append(prediction)\n        references.append(transcript)\n    except Exception as e:\n        errors.append((idx, str(e)))\n\n# Calculate WER\nwer = wer_metric.compute(predictions=predictions, references=references)\nprint(f\"Word Error Rate (WER): {wer:.4f}\")\n\n# Calculate character error rate (CER)\ndef calculate_cer(predictions, references):\n    import rapidfuzz.distance\n    cer_total = 0\n    for pred, ref in zip(predictions, references):\n        # Levenshtein distance between characters\n        distance = rapidfuzz.distance.Levenshtein.distance(pred, ref)\n        cer_total += distance / max(len(ref), 1)\n    return cer_total / len(references)\n\ncer = calculate_cer(predictions, references)\nprint(f\"Character Error Rate (CER): {cer:.4f}\")\n\n# Save results to file\nwith open('/kaggle/working/evaluation_results.txt', 'w') as f:\n    f.write(f\"Model evaluation results\\n\")\n    f.write(f\"------------------------\\n\")\n    f.write(f\"Validation samples: {len(eval_df)}\\n\")\n    f.write(f\"Successfully processed samples: {len(predictions)}\\n\")\n    f.write(f\"Word Error Rate (WER): {wer:.4f}\\n\")\n    f.write(f\"Character Error Rate (CER): {cer:.4f}\\n\")\n    f.write(f\"\\nSample predictions:\\n\")\n    for i in range(min(10, len(predictions))):\n        f.write(f\"\\nReference: {references[i]}\\n\")\n        f.write(f\"Prediction: {predictions[i]}\\n\")\n        \n    if errors:\n        f.write(f\"\\nErrors during evaluation: {len(errors)}\\n\")\n        for idx, error in errors[:10]:  # Show first 10 errors\n            f.write(f\"Sample {idx}: {error}\\n\")\n\nprint(f\"Evaluation results saved to /kaggle/working/evaluation_results.txt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}